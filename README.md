**Student:** Yewon Lim
**Student ID:** 2019145010
**University:** Yonsei University

# Large Language Diffusion Models

[![arXiv](https://img.shields.io/badge/arXiv-2502.09992-red.svg)](https://arxiv.org/abs/2502.09992)
[![deploy](https://img.shields.io/badge/Hugging%20Face%20-LLaDA_Base%20-FFEB3B)](https://huggingface.co/GSAI-ML/LLaDA-8B-Base)
[![deploy](https://img.shields.io/badge/Hugging%20Face%20-LLaDA_Instruct%20-FFEB3B)](https://huggingface.co/GSAI-ML/LLaDA-8B-Instruct)
[![deploy](https://img.shields.io/badge/ðŸ¤—%20Hugging%20Face%20-Spaces%20demo%20-blue)](https://huggingface.co/spaces/multimodalart/LLaDA)

We introduce LLaDA (**L**arge **La**nguage **D**iffusion with m**A**sking), a diffusion model with an unprecedented 8B scale, trained entirely from scratch, rivaling LLaMA3 8B in performance.

<div style="display: flex; justify-content: center; flex-wrap: wrap;">
Â  Â  <img src="./imgs/LLaDA_vs_LLaMA.svg" style="width: 45%" />
Â  Â  <img src="./imgs/LLaDA_vs_LLaMA_chat.svg" style="width: 46%" />
</div>

---

## News
- **[2024.05]** We have provided evaluation code based on the [lm-evaluation-harness](https://github.com/EleutherAI/lm-evaluation-harness) for LLaDA-Base.
- **[2024.02]** We have uploaded our paper to [arXiv](https://arxiv.org/abs/2502.09992) and open-sourced [LLaDA-8B-Base](https://huggingface.co/GSAI-ML/LLaDA-8B-Base) and [LLaDA-8B-Instruct](https://huggingface.co/GSAI-ML/LLaDA-8B-Instruct).

---

## LLaDA Training

### 1. Environment Setup

#### 1.1 Docker Container

1. Pull and run the Docker image with GPU support:

Â  Â ```bash
Â  Â docker pull pytorch/pytorch:2.5.0-cuda12.4-cudnn9-devel
Â  Â docker run --gpus all -it \
Â  Â  Â -v $(pwd):/workspace/LLaDA \
Â  Â  Â --workdir /workspace/LLaDA \
Â  Â  Â pytorch/pytorch:2.5.0-cuda12.4-cudnn9-devel bash
Â  Â ```

2. Inside the container, create and activate a Python virtual environment:

Â  Â ```bash
Â  Â python3 -m venv .venv
Â  Â source .venv/bin/activate
Â  Â ```

3. Install project dependencies and synchronize:

Â  Â ```bash
Â  Â pip install uv
Â  Â uv sync
Â  Â ```

---

### 2. Training Script (`train.sh`)

The training is orchestrated by `train.sh`. It utilizes `torchrun` for distributed training across 8 GPUs on a single node.

```bash
#!/bin/bash

torchrun \
Â  --nproc_per_node 8 \Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â # Number of GPUs per node
Â  --nnodes 1 \Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # Number of nodes
Â  -m train.finetune \Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â # Entry point module
Â  --model_name_or_path "GSAI-ML/LLaDA-8B-Instruct" \Â  # Pretrained model to fine-tune
Â  --data_path "/workspace/LLaDA/ko-gpt3_14k/data_train.jsonl" \Â  # Path to training data
Â  --bf16 True \Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â # Use bfloat16 precision
Â  --output_dir "llada_kor" \Â  Â  Â  Â  Â  Â  Â  Â  Â # Directory for checkpoints and logs
Â  --num_train_epochs 5 \Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # Total number of training epochs
Â  --per_device_train_batch_size 8 \Â  Â  Â  Â  Â  Â # Batch size per GPU for training
Â  --per_device_eval_batch_size 1 \Â  Â  Â  Â  Â  Â  # Batch size per GPU for evaluation
Â  --gradient_accumulation_steps 8 \Â  Â  Â  Â  Â  Â # Steps to accumulate gradients before updating
Â  --save_strategy "steps" \Â  Â  Â  Â  Â  Â  Â  Â  Â  # Save checkpoints every N steps
Â  --save_steps 1000 \Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â # Number of steps between checkpoint saves
Â  --save_total_limit 10 \Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â # Maximum number of checkpoints to keep
Â  --learning_rate 1e-5 \Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # Initial learning rate
Â  --weight_decay 0.1 \Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # Weight decay for optimizer
Â  --adam_beta2 0.95 \Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â # Adam optimizer beta2 parameter
Â  --warmup_ratio 0.01 \Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â # Warmup as a fraction of total steps
Â  --lr_scheduler_type "cosine" \Â  Â  Â  Â  Â  Â  Â # Learning rate scheduler type
Â  --logging_steps 10 \Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # Log metrics every N steps
Â  --report_to "wandb" \Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # Report metrics to Weights & Biases
Â  --model_max_length 512 \Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # Maximum sequence length
Â  --gradient_checkpointing False \Â  Â  Â  Â  Â  Â  # Toggle gradient checkpointing
Â  --use_lora \Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # Enable LoRA parameter-efficient fine-tuning
Â  --deepspeed deepspeed/zero2.json \Â  Â  Â  Â  Â  # DeepSpeed config for Zero2 optimization
Â  --label_names prompt_lengthsÂ  Â  Â  Â  Â  Â  Â  Â  Â # Input field names used as labels
```
